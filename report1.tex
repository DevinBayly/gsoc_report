\subsection{Abstract}\label{abstract}

\twocolumn [
State of the art computational tools are in especially high demand in
the field of Computational Neuroscience. However, bottleneck exists in
terms of how much data can be transferred between hard disk and memory
for computation. There are really two issues here: the transfer of data
from disk to ram, and subsequently ram to cache means ram throttles
performance on both ends; current ram capacity is many magnitudes
smaller than what is needed to model the human brain, which constrains
the size of simulations that can be run. One strategy which addresses
this is memory compression of data in memory (DRAM). With these
manipulations the structure holding our data will use less ram to
transfer between disk and cpu caches. This project is intended to
explore potential solution to the memory bottleneck problem in
neuroscience simulations. These solutions will be prototyped within the
Neuromapp program before any attempts are made to apply them to the
larger and more complicated BBP codebase. It appears that this
compression doesn't provide significant speedups, but does
siginificantly decrease the program's memory footprint.
]

\subsection{Introduction}\label{introduction}

Scientific investigation increasingly relies on computation, and as such
it is constrained by hardware limitations. In particular one bottleneck
plagues researchers in many diverse disciplines: the RAM bottleneck.
\textbf{cite} Mao et al. describe \emph{RAM-Latency} Dominated
applications as an actual category of programs. These programs may
require datastructures so large the CPU on-chip cache will have finished
its calculation before the RAM has a chance to curry results to the
disk, and new data to the CPU (??). They then outline that ``batching,
sorting, and IO concurrency'' are general purpose solutions that can
relieve this bottle neck. Vivek et al. describe RAM bottlenecks as an
impediment to Next Generation Sequencing of genomes, and encourage task
decomposition as a means of recovering performance. This problem affects
researchers across the board when large operations are concerned.

In order to deal with this bottle neck we have options worth exploring.
The two references above have their own suggestions for workarounds, but
neither mention compression as an alternative. The concept here is that
by compressing sections of the data that aren't currently needed for
computation you can decrease the footprint in memory. By decreasing this
footprint it is possible to receive more calculations per cycle. A
popular choice for compression tool is Zlib designed by Jean-Loup Gailly
and Mark Adler \textbf{cite}. While compression with Zlib is far from
unique, applying it to relieving bottlenecks in computational
neuroscience appears to be a novel endeavor.

\subsection{Materials and Methods}\label{materials-and-methods}

Let us now consider the details of the implementation starting with the
block container. The block is a representation of a contiguous piece of
memory. By adopting a convention that a unique block address can be
described by the row position multiplied by the column position, we can
transform a large 1 dimensional array of memory into a 2 dimensional
representation; In this way the block may be treated like a general
matrix. Worth noting upfront is the decision to embrace a policy design
for the block, and its essential components. This means that a block
particulars -- compressor, allocator, or type -- can be specified in the
instantiation. There are two varieties of allocator: cstandard is for
regular malloc and free memory allocation; align is for posix memory
allocation. Zlib is our currently implemented compression policy, but
others can be added using zlib policy as a template. The numeric value
stored in the block may be any one of the c++ numeric types, and this is
accomplished through standard templating terms. These items make up the
foundational aspects of the block.

With this understanding in place, its worth discussing the functional
capabilities of the block. The block supports basic IO using the c++
stream redirection operator \textless{}\textless{} for output via
ostream types, and input \textgreater{}\textgreater{} operations in
relation to ifstream types. Continuing with STL related capabilities,
the block also has a nested random access iterator class which enables
more straightforward access to other tools in the STL, such as sorting.
The compression capabilities of the block allow for one shot full block
compression/uncompression using the zlib library. It is possible to to
interrogate the block for information relating to its size, or current
compression state. This layer is essential for using the block in higher
level programs like those discussed below.

Lets now discuss the various tools included in the compression mini-app
that leverage the block. Following the established pattern, the
compression mini-app features a command line program: \textbf{use code
markdown} ./app compression --compression --file \{file\_arg\} --split
--sort --benchmark --stream\_benchmark --kernel\_measure are all
options. The --compression flag runs a standard single file routine on
the --file \{file\_arg\} if provided. The --split routine may be added
in to parse the double, or float numeric type into its binary
representation sorted by category: sign, sign, \ldots{} ,exponent,
exponent, \ldots{} ,mantissa, mantissa \ldots{} . The --sort option
orders all of the columns in the block based on the values specified in
a particular row of the block. The --benchmark is the default, and
follows through with a compression, split,and sort combo run on a
default specified file. The --stream\_benchmark option initiates the
block hybrid of the STREAM\_BENCHMARK bandwith measurement test designed
by John McCalpin at University of Virginia. Last but not least, there is
the --kernel\_measure option which compares compression and
non-compression performance changes as a function of increasing levels
of computational complexity. This largely concludes the upper level tour
of the functionality --both old and new-- that comes with the block, and
the compression mini-app.

Next lets briefly consider a few implementation details used to achieve
this functionality. Programs of this size typically will use an
automated build tool, for Neuromapp that tool is CMake. Currently the
build is setup to disable the compression app by default to prevent
issues with the Blue Queen (??). When enabled, Cmake for the compression
mini-app can create a subdirectory in the \$\{Binary Root\} (out of
source build directory root) named neuromapp/compression/\emph{. This
file contains the compiled binaries necessary for running the
compression mini-app, and sourcing input files from the
neuromapp/test/block\_data/} path. Time is often our enemy, but it was
put to work extensively in this project. The compression mini-app now
has a timer tool that is very useful for profiling small chunks of code,
and provides duration counts in miliseconds. Code developed in this
project followed a general policy design strategy. A major benefit of
this approach is there now exists a framework for adding in additional
compression libraries in the future which will be discussed later on.
The compression policy is an intrinsic part of the block and defaults to
zlib compression. Utility functions ``compress/uncompress''
\textbf{code} are used for the respective one-shot operations on the
block one-shot meaning they compress, and uncompress entirely.

The stream benchmark is similar to the compression in that it operates
on the block contents in lower level terms. This tool measures bandwith
during four canonical computations copy, scale, add, triad involving
numeric containers (labeled A,B,C): copy asigns the contents of A to B;
scale asigns scalar multiples of each element in A to B; add sums A and
B elementwise and assigns to C; the triad assigns to C the elementwise
sum of A and B multiplied by a scalar. The block hybrid uses A,B,C
vectors containing 640 blocks of 8000 bytes each to bring the actual
transfer sizes to reasonable levels. In each of the computations
described above we loop over all of the vector positions using the timer
to calculate run times for the whole set. Each calculation is performed
on blocks that involve the compression routine, and those that don't for
comparison. This block hybrid is based on the original stream benchmark
created by John D McCalpin, and is a useful starting metric of
compression effects on performance.

Another performance evaluation tool used by the compression mini-app is
the kernel measure. This uses three increasingly complex calculations to
explore performance differences between compression , and non
compression routines as a function of complexity. The first level is a
simple addition of ints, meant to be the least complex computation which
is still a computation; the second level is meant to resemble the
calculation that is performed in updating synapses PSP's using the
Tsodyks-Markram model treating the blocks data as parameters for the
main formulae; the third level is a Euler method for solving a
differential equation where the equation has been specified as
\textbf{latex} y\^{}2 + 30*t arbitrarily. We use a vector of 100
identical blocks in our program to ensure that each block is not simply
left in a cache between each of these operations. Altogether this tool
should help provide another dimension to the question of performance
improvements, as we can tell when the compression is effectively offset
by the complexity of a calculation.

The extensive BOOST library is used to assist in argument parsing, and
testing. The program\_options tool is used to provide program help
options, and parse the arguments provided by the user in a flexible
manner. BOOST allows for creation of a full suite of unit testing.
Testing was applied to ensure that the process of reading into a file
created the same block representations as if values had been provided
one by one for each block element. Another important domain for testing
was to determine that none of the values in the block are modified
permanently as a result of the compression/uncompression routines. With
this all captured, it makes sense to move to the results section of this
report.

\subsection{Results}\label{results}

\subsection{Discussion}\label{discussion}

\subsection{Conclusion}\label{conclusion}
